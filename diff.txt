diff --git a/new_test.py b/main.py
similarity index 66%
rename from new_test.py
rename to main.py
index 22567cc..6477c34 100644
--- a/new_test.py
+++ b/main.py
@@ -1,5 +1,6 @@
 import argparse
 import time
+from src.util.logger import set_logger
 
 import pytorch_lightning as pl
 from torch.utils.data import DataLoader
@@ -16,7 +17,6 @@ from src.customtest import Tester
 
 from src.util.preprocessor import Preprocessor
 
-import optuna
 from optuna.samplers import GridSampler
 import numpy as np
 import random
@@ -31,7 +31,7 @@ parser.add_argument('--lr', type=float, default=0.001,             help='Learnin
 parser.add_argument('--weight_decay', type=float, default=0.00001, help='Weight decay(for both FM and autoencoder)')
 parser.add_argument('--num_epochs_training', type=int, default=100,help='Number of epochs')
 parser.add_argument('--batch_size', type=int, default=4096,        help='Batch size')
-parser.add_argument('--num_workers', type=int, default=10,         help='Number of workers for dataloader')
+parser.add_argument('--num_workers', type=int, default=10,          help='Number of workers for dataloader')
 parser.add_argument('--num_deep_layers', type=int, default=2,      help='Number of deep layers')
 parser.add_argument('--deep_layer_size', type=int, default=128,    help='Size of deep layers')
 parser.add_argument('--seed', type=int, default=42)
@@ -47,12 +47,10 @@ parser.add_argument('--cont_dims', type=int, default=0,            help='continu
 parser.add_argument('--shopping_file_num', type=int, default=147,  help='name of shopping file choose from 147 or  148 or 149')
 
 parser.add_argument('--datatype', type=str, default="ml100k",           help='ml100k or ml1m or shopping or goodbook or frappe')
-parser.add_argument('--isuniform', type=bool, default=False,             help='true if uniform false if not')
+parser.add_argument('--isuniform', type=bool, default=False,            help='true if uniform false if not')
 parser.add_argument('--embedding_type', type=str, default='original',   help='SVD or NMF or original')
 parser.add_argument('--model_type', type=str, default='fm',             help='fm or deepfm')
 
-args = parser.parse_args("")
-
 # seed 값 고정
 def setseed(seed: int):
     random.seed(seed)
@@ -84,11 +82,9 @@ def getdata(args):
 
 
 def trainer(args, data: Preprocessor):
-
     cats, conts = data.cat_train_df, data.cont_train_df
     target, c = data.target, data.c
     field_dims = data.field_dims
-    uidf = data.uidf.values
 
     # I know this is a bit inefficient to create all four classes for model, but I did this for simplicity
     if args.model_type=='fm' and args.embedding_type=='original':
@@ -98,85 +94,94 @@ def trainer(args, data: Preprocessor):
     elif args.model_type=='deepfm' and args.embedding_type=='original':
         model = DeepFM(args, field_dims)
         Dataset = CustomDataLoader(cats, conts, target, c)
-
+    
     elif args.model_type=='fm':
         model = FMSVD(args, field_dims)
-        embs = conts[:, -args.num_eigenvector*2:]   # Here, numeighenvector*2 refers to embeddings for both user and item
+        svd_embs = conts[:, -args.num_eigenvector*2:]   # Here, numeighenvector*2 refers to embeddings for both user and item
         conts = conts[:, :-args.num_eigenvector*2]  # rest of the columns are continuous columns (e.g. age, , etc.)
-        Dataset = SVDDataloader(cats, embs, uidf, conts, target, c)
+        Dataset = SVDDataloader(cats, svd_embs, conts, target, c)
 
     elif args.model_type=='deepfm':
         model = DeepFMSVD(args, field_dims)
-        embs = conts[:, -args.num_eigenvector*2:]   # Here, numeighenvector*2 refers to embeddings for both user and item
+        svd_embs = conts[:, -args.num_eigenvector*2:]   # Here, numeighenvector*2 refers to embeddings for both user and item
         conts = conts[:, :-args.num_eigenvector*2]  # rest of the columns are continuous columns (e.g. age, , etc.)
-        Dataset = SVDDataloader(cats, embs, uidf, conts, target, c)
+        Dataset = SVDDataloader(cats, svd_embs, conts, target, c)
 
     else:
         raise NotImplementedError
     
-    dataloader = DataLoader(Dataset, batch_size=args.batch_size, shuffle=True, num_workers=20)
+    dataloader = DataLoader(Dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)
     
     start = time.time()
-    trainer = pl.Trainer(max_epochs=args.num_epochs_training, enable_checkpointing=False, logger=False)
+    trainer = pl.Trainer(
+        max_epochs=args.num_epochs_training, 
+        enable_checkpointing=False, 
+        devices=1,
+        strategy='single_device',
+        logger=False)
     trainer.fit(model, dataloader)
     end = time.time()
     return model, end-start
 
 # This is code for multiple experiments
-def objective(trial: optuna.trial.Trial) :
-    args = parser.parse_args("")
-    args.embedding_type = trial.suggest_categorical('embedding_type', ['original', 'SVD'])
-    args.model_type = trial.suggest_categorical('model_type', ['fm', 'deepfm'])
-
-    model_desc = args.embedding_type + args.model_type
-    print("model is :", model_desc)
-    seeds = [42]
-    scores = []
-    for seed in seeds:
-        setseed(seed=seed)
-        data_info = getdata(args)
-
-        model, timeee = trainer(args, data_info)
-        tester = Tester(args, model, data_info)
-
-        result = tester.test()
+# def objective(trial: optuna.trial.Trial) :
+#     args = parser.parse_args("")
+#     args.embedding_type = trial.suggest_categorical('embedding_type', ['original', 'SVD'])
+#     args.model_type = trial.suggest_categorical('model_type', ['fm', 'deepfm'])
 
-        global result_dict
-        result_dict = result_checker(result_dict, result, model_desc)
-        scores.append(result['precision'])
+#     model_desc = args.embedding_type + args.model_type
+#     print("model is :", model_desc)
+#     seeds = [42]
+#     scores = []
+#     for seed in seeds:
+#         setseed(seed=seed)
+#         data_info = getdata(args)
 
-    return result['precision']
+#         model, timeee = trainer(args, data_info)
+#         tester = Tester(args, model, data_info)
 
-result_dict = {}
+#         result = tester.test()
 
-search_space = {'embedding_type' : ['SVD'], 'model_type' : ['fm', 'deepfm']}
-sampler = GridSampler(search_space)
-study = optuna.create_study(sampler=sampler)
-study.optimize(objective, n_trials=4)
+#         global result_dict
+#         result_dict = result_checker(result_dict, result, model_desc)
+#         scores.append(result['precision'])
 
-for models in result_dict.keys():
-    print(models)
-    print(result_dict[models]['precision'])
+#     return result['precision']
 
-# with open('results/sparseSVD_deepfm.pickle', mode='wb') as f:
-#     pickle.dump(result_dict, f)
+# result_dict = {}
 
-# # This is for one-time run
-# if __name__=='__main__':
-#     setseed(seed=42)
-#     args = parser.parse_args("")
-#     results = {}
-#     data_info = getdata(args)
+# search_space = {'embedding_type' : ['SVD'], 'model_type' : ['deepfm']}
+# sampler = GridSampler(search_space)
+# study = optuna.create_study(sampler=sampler)
+# study.optimize(objective, n_trials=4)
 
-#     print('model type is', args.model_type)
-#     print('embedding type is', args.embedding_type)
-#     model, timeee = trainer(args, data_info)
-#     test_time = time.time()
-#     tester = Tester(args, model, data_info)
+# for models in result_dict.keys():
+#     print(models)
+#     print(result_dict[models]['precision'])
+#     print(result_dict[models]['time'])
 
-#     result = tester.test()
 
-#     end_test_time = time.time()
-#     results[args.embedding_type + args.model_type] = result
-#     print(results)
-#     print("time :", timeee)
\ No newline at end of file
+# This is for one-time run
+if __name__=='__main__':
+    setseed(seed=42)
+    torch.set_float32_matmul_precision('high')
+    logger = set_logger()
+    args = parser.parse_args("")
+    
+    results = {}
+    args.embedding_type = 'SVD'
+    args.model_type = 'fm'
+    preprocessor = getdata(args)
+
+    logger.info('model type is %s', args.model_type)
+    logger.info('embedding type is %s', args.embedding_type)
+    model, timeee = trainer(args, preprocessor)
+    test_time = time.time()
+    tester = Tester(args, model, preprocessor)
+
+    result = tester.test()
+
+    end_test_time = time.time()
+    results[args.embedding_type + args.model_type] = result
+    logger.info(results)
+    logger.info("time : %s", timeee)
\ No newline at end of file
diff --git a/src/customtest.py b/src/customtest.py
index 72ab97d..899dfa6 100644
--- a/src/customtest.py
+++ b/src/customtest.py
@@ -1,9 +1,11 @@
 import pandas as pd
 import numpy as np
-# from src.data_util.fm_preprocess import FM_Preprocessing
 from src.util.preprocessor import Preprocessor
 import tqdm
 import torch
+import logging
+
+logger = logging.getLogger("svdfm_test")
 
 class Tester:
 
@@ -53,7 +55,8 @@ class Tester:
         user_list = self.le_dict['user_id'].classes_
         self.model.eval()
         precisions, recalls, hit_rates, reciprocal_ranks, dcgs = [], [], [], [], []
-
+        
+        logger.info("Starting testing...")
         for customerid in tqdm.tqdm(user_list[:]):
 
             if customerid not in self.test_org['user_id'].unique():
@@ -63,12 +66,13 @@ class Tester:
             X_cat = torch.tensor(cur_user_df[self.cat_cols].values, dtype=torch.int64)
             X_cont = torch.tensor(cur_user_df[self.cont_cols].values, dtype=torch.float32)
 
-            if self.args.embedding_type=='original' and self.args.model_type=='fm':
+            if self.args.embedding_type=='original':
                 emb_x = self.model.embedding(X_cat)
-                result, _, _, _ = self.model.forward(X_cat, X_cont, emb_x)
+                result, _, _, _ = self.model.forward(X_cat, emb_x, X_cont)
 
-            elif self.args.embedding_type=='original' and self.args.model_type=='deepfm':
-                result = self.model.forward(X_cat, X_cont)
+            # elif self.args.embedding_type=='original' and self.args.model_type=='deepfm':
+            #     emb_x = self.model.embedding(X_cat)
+            #     result = self.model.forward(X_cat, emb_x, X_cont)
 
             else:
                 svd_emb = X_cont[:, -self.args.num_eigenvector*2:]
@@ -91,11 +95,9 @@ class Tester:
             hit_rates.append(self.get_hit_rate(pred, real))
             reciprocal_ranks.append(self.get_reciprocal_rank(pred, real))
             dcgs.append(self.get_dcg(pred, real))
+        
+        logger.info("Completed testing")
 
-        print("average precision: ",np.mean(precisions))
-        # total user number and total item number
-        # print("total user number: ",len(user_list))
-        # print("total item number: ",len(self.train_df['item_id'].unique()))
         metrics = {'precision' : np.mean(precisions), 
                    'recall' : np.mean(recalls),
                    'hit_rate' : np.mean(hit_rates),
diff --git a/src/data_util/dataloader_SVD.py b/src/data_util/dataloader_SVD.py
index 893c536..7183e23 100644
--- a/src/data_util/dataloader_SVD.py
+++ b/src/data_util/dataloader_SVD.py
@@ -1,20 +1,20 @@
 import torch.utils.data as data_utils
+import numpy as np
 
-## dataloader for SVD + FM based model part. 
+## dataloader for SVD
 
 class SVDDataloader(data_utils.Dataset):
     # as we already converted to tensor, we can directly return the tensor
-    def __init__(self, x, emb, ui, cons, y, c) -> None:
+    def __init__(self, x: np.ndarray, svd_emb: np.ndarray, cons: np.ndarray, y: np.ndarray, c: np.ndarray) -> None:
         self.x = x
-        self.emb = emb
+        self.svd_emb = svd_emb
         self.cons = cons
         self.y = y
         self.c = c
-        self.ui = ui
         super().__init__()
     
     def __len__(self):
         return len(self.x)
     
     def __getitem__(self, index):
-        return self.x[index], self.emb[index], self.ui[index], self.cons[index], self.y[index], self.c[index]
\ No newline at end of file
+        return self.x[index], self.svd_emb[index], self.cons[index], self.y[index], self.c[index]
\ No newline at end of file
diff --git a/src/model/SVD_emb/deepfmsvd.py b/src/model/SVD_emb/deepfmsvd.py
index 95d0931..c639f29 100644
--- a/src/model/SVD_emb/deepfmsvd.py
+++ b/src/model/SVD_emb/deepfmsvd.py
@@ -9,7 +9,6 @@ from src.model.SVD_emb.layers import FeatureEmbedding, FeatureEmbedding, FM_Line
 #from src.util.scaler import StandardScaler
 
 
-
 class DeepFMSVD(pl.LightningModule):
     def __init__(self, args,field_dims):
         super(DeepFMSVD, self).__init__()
@@ -39,37 +38,36 @@ class DeepFMSVD(pl.LightningModule):
         return loss_y
     
 
-    def forward(self, x,embed_x,svd_emb,x_cont):
+    def forward(self, x, embed_x, svd_emb, x_cont):
         # FM part, here, x_hat means another arbritary input of data, for combining the results. 
-        fm_part, cont_emb, lin_term, inter_term=self.fm(x, embed_x, svd_emb, x_cont)
+        _, cont_emb, lin_term, inter_term = self.fm(x, embed_x, svd_emb, x_cont)
         user_emb = svd_emb[:, :self.args.num_eigenvector]
         item_emb = svd_emb[:, self.args.num_eigenvector:]
         
-        embed_x = torch.cat((embed_x,cont_emb), 1)
+        embed_x = torch.cat((embed_x, cont_emb), 1)
         feature_number = embed_x.shape[1]
         
         # to make embed_x to batch_size * (num_features*embedding_dim)
         embed_x = embed_x.reshape(-1, feature_number*self.args.emb_dim)
         
-        new_x = torch.cat((embed_x, user_emb),1)
-        new_x = torch.cat((new_x, item_emb),1)
+        new_x = torch.cat((embed_x, user_emb, item_emb),1)
+        # new_x = torch.cat((new_x, item_emb),1)
         deep_part = self.mlp(new_x)
         
         # Deep part
-        lin_term = self.sig(lin_term)
-        inter_term = self.sig(inter_term)
-        deep_part = self.sig(deep_part)
+        lin_term_sig = self.sig(lin_term)
+        inter_term_sig = self.sig(inter_term)
+        deep_part_sig = self.sig(deep_part)
 
-        outs = torch.cat((lin_term, inter_term ), 1)
-        outs = torch.cat((outs, deep_part), 1)
+        outs = torch.cat((lin_term_sig, inter_term_sig, deep_part_sig), 1)
         y_pred = self.lastlinear(outs).squeeze(1)
 
         return y_pred
 
     def training_step(self, batch):
-        x, svd_emb, ui, x_cont, y, c_values = batch
+        x, svd_emb, x_cont, y, c_values = batch
         embed_x = self.embedding(x)
-        y_pred = self.forward(x,embed_x, svd_emb, x_cont)
+        y_pred = self.forward(x, embed_x, svd_emb, x_cont)
         loss_y = self.loss(y_pred, y, c_values)
         self.log('train_loss', loss_y, on_step=True, on_epoch=True, prog_bar=True, logger=True)
         return loss_y
diff --git a/src/model/SVD_emb/fmsvd.py b/src/model/SVD_emb/fmsvd.py
index 405094f..45cab3c 100644
--- a/src/model/SVD_emb/fmsvd.py
+++ b/src/model/SVD_emb/fmsvd.py
@@ -29,7 +29,7 @@ class FMSVD(pl.LightningModule):
     
     def forward(self, x, emb_x, svd_emb, x_cont):
         # x: batch_size * num_features
-        lin_term = self.linear(x=x, emb_x=svd_emb, x_cont=x_cont)
+        lin_term = self.linear(x=x, svd_emb=svd_emb, x_cont=x_cont)
         inter_term, cont_emb = self.interaction(emb_x=emb_x, svd_emb=svd_emb, x_cont=x_cont)
         # to normalize lin_term and inter_term to be in the same scale
         # so that the weights can be comparable
@@ -41,9 +41,9 @@ class FMSVD(pl.LightningModule):
         return x, cont_emb, lin_term, inter_term
 
     def training_step(self, batch, batch_idx):
-        x, svd_emb, ui, x_cont, y, c_values = batch
-        embed_x = self.embedding(x)
-        y_pred, _, _, _ = self.forward(x=x, emb_x=embed_x, svd_emb=svd_emb, x_cont=x_cont)
+        x, svd_emb, x_cont, y, c_values = batch
+        emb_x = self.embedding(x)
+        y_pred, _, _, _ = self.forward(x=x, emb_x=emb_x, svd_emb=svd_emb, x_cont=x_cont)
         loss_y = self.loss(y_pred, y, c_values)
         self.log('train_loss', loss_y, on_step=True, on_epoch=True, prog_bar=True, logger=True)
         return loss_y
diff --git a/src/model/SVD_emb/layers.py b/src/model/SVD_emb/layers.py
index daf687b..dc7e547 100644
--- a/src/model/SVD_emb/layers.py
+++ b/src/model/SVD_emb/layers.py
@@ -33,7 +33,6 @@ class FeatureEmbedding(nn.Module):
         self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)
 
     def forward(self, x):
-        # input x: batch_size * num_features
         x = x + x.new_tensor(self.offsets).unsqueeze(0)  # this is for adding offset for each feature for example, movie id starts from 0, user id starts from 1000
         x = self.embedding(x)
         return x
@@ -48,15 +47,15 @@ class FM_Linear(nn.Module):
         self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)
         self.args = args
             
-    def forward(self, x, emb_x, x_cont):
+    def forward(self, x, svd_emb, x_cont):
         # input x: batch_size * num_features
         x = x + x.new_tensor(self.offsets).unsqueeze(0)
         linear_term = self.linear(x)
         cont_linear = torch.matmul(x_cont, self.w).reshape(-1, 1)
 
         # added because of svd_embed
-        user_emb = emb_x[:, 0].unsqueeze(1).unsqueeze(1)
-        item_emb = emb_x[:, self.args.num_eigenvector].unsqueeze(1).unsqueeze(1)
+        user_emb = svd_emb[:, 0].unsqueeze(1).unsqueeze(1)
+        item_emb = svd_emb[:, self.args.num_eigenvector].unsqueeze(1).unsqueeze(1)
         nemb_x = torch.cat((user_emb, item_emb), 1)
         linear_term = torch.cat((linear_term, nemb_x), 1)
 
@@ -73,19 +72,19 @@ class FM_Interaction(nn.Module):
         self.v = nn.Parameter(torch.randn(args.cont_dims-args.num_eigenvector*2, args.emb_dim))
     
     def forward(self, emb_x, svd_emb, x_cont):
-        x_cont = x_cont.unsqueeze(1)
+        
         user_emb = svd_emb[:, :self.args.num_eigenvector].unsqueeze(1)
         item_emb = svd_emb[:, self.args.num_eigenvector:].unsqueeze(1)
-        x_comb = torch.cat((emb_x, user_emb), 1)
-        x_comb = torch.cat((x_comb, item_emb), 1)
+        x_comb = torch.cat((emb_x, user_emb, item_emb), 1)
+        x_cont = x_cont.unsqueeze(1)
 
         cont = torch.matmul(x_cont, self.v)
         x_comb = torch.cat((x_comb, cont), 1)
 
-        sum_square = torch.sum(x_comb, 1)**2
-        square_sum = torch.sum(x_comb**2, 1)
+        linear = torch.sum(x_comb, dim=1)**2
+        interaction = torch.sum(x_comb**2, dim=1)
         
-        interaction = 0.5*torch.sum(sum_square-square_sum, 1, keepdim=True)
+        interaction = 0.5*torch.sum(linear-interaction, 1, keepdim=True)
         cont_emb = self.v.unsqueeze(0).repeat(x_comb.shape[0], 1, 1)
         
         return interaction, cont_emb
\ No newline at end of file
diff --git a/src/model/original/deepfm.py b/src/model/original/deepfm.py
index 9916601..42dfccc 100644
--- a/src/model/original/deepfm.py
+++ b/src/model/original/deepfm.py
@@ -32,32 +32,30 @@ class DeepFM(pl.LightningModule):
         loss_y = weighted_bce.mean()
         return loss_y
     
-    def forward(self, x, x_cont):
+    def forward(self, x, emb_x, x_cont):
         # FM part, here, x_hat means another arbritary input of data, for combining the results. 
-        
-        embed_x = self.embedding(x)
-        fm_part, cont_emb, lin_term, inter_term = self.fm.forward(x, x_cont, embed_x)
+        _, cont_emb, lin_term, inter_term = self.fm.forward(x, emb_x, x_cont)
         
         if cont_emb is not None:
-            embed_x = torch.cat((embed_x, cont_emb), 1)
-        feature_number = embed_x.shape[1]
-        embed_x = embed_x.view(-1, feature_number * self.args.emb_dim)
+            emb_x = torch.cat((emb_x, cont_emb), 1)
+        feature_number = emb_x.shape[1]
+        emb_x = emb_x.view(-1, feature_number * self.args.emb_dim)
 
-        new_x = embed_x
-        deep_part = self.mlp(new_x)
+        new_x = emb_x
+        mlp_x = self.mlp(new_x)
 
-        lin_term = self.sig(lin_term)
-        inter_term = self.sig(inter_term)
-        deep_part = self.sig(deep_part)
-        outs = torch.cat((lin_term,inter_term), 1)
-        outs = torch.cat((outs,deep_part), 1)
+        lin_term_sig = self.sig(lin_term)
+        inter_term_sig = self.sig(inter_term)
+        mlp_term_sig = self.sig(mlp_x)
+        outs = torch.cat((lin_term_sig, inter_term_sig, mlp_term_sig), dim=1)
         y_pred = self.lastlinear(outs).squeeze(1)
 
-        return y_pred
+        return y_pred, cont_emb, lin_term, inter_term
 
     def training_step(self, batch, batch_idx):
         x, x_cont, y, c_values = batch
-        y_pred = self.forward(x, x_cont)
+        embed_x = self.embedding(x)
+        y_pred, _, _, _ = self.forward(x, embed_x, x_cont)
         loss_y = self.loss(y_pred, y,c_values)
         self.log('train_loss', loss_y, on_step=True, on_epoch=True, prog_bar=True, logger=True)
         return loss_y
diff --git a/src/model/original/fm.py b/src/model/original/fm.py
index f52d945..5f207d8 100644
--- a/src/model/original/fm.py
+++ b/src/model/original/fm.py
@@ -5,6 +5,7 @@ from src.model.original.layers import FeatureEmbedding, FM_Linear, FM_Interactio
 
 import pytorch_lightning as pl
 from itertools import chain
+from torchviz import make_dot
 
 class FM(pl.LightningModule):
     def __init__(self, args, field_dims):
@@ -27,23 +28,24 @@ class FM(pl.LightningModule):
         loss_y = weighted_bce.mean()
         return loss_y 
     
-    def forward(self, x, x_cont, emb_x):
+    def forward(self, x, emb_x, x_cont):
         # FM part loss with interaction terms
         # x: batch_size * num_features
         lin_term = self.linear(x=x, x_cont=x_cont)
         inter_term, cont_emb = self.interaction(emb_x, x_cont)
+        
         lin_term_sig = self.sig(lin_term)
         inter_term_sig = self.sig(inter_term)
         outs = torch.cat((lin_term_sig, inter_term_sig), 1)
-        x = self.last_linear(outs)
-        x = x.squeeze(1)
+        y_pred = self.last_linear(outs)
+        y_pred = y_pred.squeeze(1)
             
-        return x, cont_emb, lin_term, inter_term
+        return y_pred, cont_emb, lin_term, inter_term
     
     def training_step(self, batch, batch_idx):
         x, x_cont, y, c_values = batch
         embed_x = self.embedding(x)
-        y_pred, _, _, _ = self.forward(x, x_cont, embed_x)
+        y_pred, _, _, _ = self.forward(x, embed_x, x_cont)
         loss_y = self.loss(y_pred, y, c_values)
         self.log('train_loss', loss_y, on_step=True, on_epoch=True, prog_bar=True, logger=True)
         return loss_y
diff --git a/src/model/original/layers.py b/src/model/original/layers.py
index 56476f9..8e1323a 100644
--- a/src/model/original/layers.py
+++ b/src/model/original/layers.py
@@ -23,7 +23,9 @@ class MLP(nn.Module):
         return x
 
 class FeatureEmbedding(nn.Module):
-
+    """
+    dtype==torch.int64인 x의 각 integer를 16차원의 embedding vector로 변환
+    """
     def __init__(self, args, field_dims):
         super(FeatureEmbedding, self).__init__()
         self.embedding = nn.Embedding(sum(field_dims+1), args.emb_dim)
@@ -35,9 +37,13 @@ class FeatureEmbedding(nn.Module):
         x = self.embedding(x)
         return x
 
-
 class FM_Linear(nn.Module):
-
+    """
+    dtype==torch.int64인 x의 각 integer를 1차원의 embedding vector로 변환
+    같은 데이터 행(row)별 합을 구한 후 cont_linear과 합침
+    i.e. (N개의 row, k개의 column이 있는 x가 있을 때)
+         (N, k) -> (N, k, 1) (embedding 추가) -> (N, 1) 
+    """
     def __init__(self, args, field_dims):
         super(FM_Linear, self).__init__()
         self.linear = torch.nn.Embedding(sum(field_dims)+1, 1)
@@ -51,8 +57,8 @@ class FM_Linear(nn.Module):
         linear_term = self.linear(x)
         cont_linear = torch.matmul(x_cont, self.w).reshape(-1, 1) # add continuous features
         
-        x = torch.sum(linear_term, dim=1) + self.bias
-        x = x + cont_linear 
+        x = torch.sum(linear_term, dim=1) + self.bias # 각 row마다 합을 구함 
+        x = x + cont_linear
         return x
 
 class FM_Interaction(nn.Module):
@@ -65,15 +71,28 @@ class FM_Interaction(nn.Module):
     def forward(self, x, x_cont):
         x_comb = x
         x_cont = x_cont.unsqueeze(1)
-        linear = torch.sum(x_comb, 1)**2
-        interaction = torch.sum(x_comb**2, 1)
-        if self.args.cont_dims!=0:
-            cont_linear = torch.sum(torch.matmul(x_cont, self.v)**2, dim=1)
-            linear = torch.cat((linear, cont_linear), 1)
-            cont_interaction = torch.sum(torch.matmul(x_cont**2, self.v**2), 1, keepdim=True)
-            interaction = torch.cat((interaction, cont_interaction.squeeze(1)), 1)
 
+        cont = torch.matmul(x_cont, self.v)
+        x_comb = torch.cat((x_comb, cont), 1)
+
+        linear = torch.sum(x_comb, dim=1)**2
+        interaction = torch.sum(x_comb**2, dim=1)
+        
         interaction = 0.5*torch.sum(linear-interaction, 1, keepdim=True)
         cont_emb = self.v.unsqueeze(0).repeat(x_comb.shape[0], 1, 1)
+        
+        return interaction, cont_emb
+        # x_comb = x
+        # x_cont = x_cont.unsqueeze(1)
+        # linear = torch.sum(x_comb, dim=1)**2
+        # interaction = torch.sum(x_comb**2, dim=1)
+        # if self.args.cont_dims!=0:
+        #     cont_linear = torch.sum(torch.matmul(x_cont, self.v)**2, dim=1)
+        #     linear = torch.cat((linear, cont_linear), 1)
+        #     cont_interaction = torch.sum(torch.matmul(x_cont**2, self.v**2), 1, keepdim=True)
+        #     interaction = torch.cat((interaction, cont_interaction.squeeze(1)), 1)
+
+        # interaction = 0.5*torch.sum(linear-interaction, 1, keepdim=True)
+        # cont_emb = self.v.unsqueeze(0).repeat(x_comb.shape[0], 1, 1)
 
-        return interaction, cont_emb
\ No newline at end of file
+        # return interaction, cont_emb
\ No newline at end of file
diff --git a/src/util/embed_SparseSVD.py b/src/util/embed_SparseSVD.py
index 0077fa3..629c1bf 100644
--- a/src/util/embed_SparseSVD.py
+++ b/src/util/embed_SparseSVD.py
@@ -1,4 +1,7 @@
 from sklearn.decomposition import MiniBatchSparsePCA
+import logging
+
+logger = logging.getLogger("svdfm_test")
 
 class embed_SparseSVD:
 
@@ -11,15 +14,15 @@ class embed_SparseSVD:
         sparse matrix(x)와 number of singular values(k)를 입력받으면 
         SVD 행렬분해 수행
         """
-        print("pca 1 start")
+        logger.info("pca 1 start")
         pca1 = MiniBatchSparsePCA(n_components=self.args.num_eigenvector, 
                                   alpha=1, batch_size=100)
         pca1.fit(x@x.T)
         u = pca1.components_
-        print("pca 2 start")
+        logger.info("pca 2 start")
         pca2 = MiniBatchSparsePCA(n_components=self.args.num_eigenvector, 
                                   alpha=1, batch_size=100)
         pca2.fit(x.T@x)
         v = pca2.components_
-
+        logger.info('pca ended')
         return u.T, v.T
\ No newline at end of file
diff --git a/src/util/logger.py b/src/util/logger.py
new file mode 100644
index 0000000..768a319
--- /dev/null
+++ b/src/util/logger.py
@@ -0,0 +1,41 @@
+import logging
+import os
+import sys
+from datetime import datetime, timedelta, timezone
+from multiprocessing import current_process
+
+
+def set_logger(log_dir: str = './logs'):	
+    # 현지 시각 기준으로 타임스탬프 설정
+    kst = timezone(timedelta(hours=9))
+    now = datetime.now(kst)
+    timestamp = now.strftime('%Y%m%d_%H%M%S')
+
+    logger = logging.getLogger('svdfm_test')
+    logger.setLevel(logging.INFO)
+
+    # 중복 방지
+    if logger.handlers:
+        return logger
+    logger.propagate = False
+
+    rank_env = os.environ.get('LOCAL_RANK') or os.environ.get('RANK') or os.environ.get('GLOBAL_RANK')
+    rank = int(rank_env) if rank_env is not None else 0
+
+    # 포매팅
+    fmt = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y%m%d %H:%M:%S')
+
+    if current_process().name == 'MainProcess' and rank==0:
+        # 로그 저장 폴더
+        os.makedirs(log_dir, exist_ok=True)
+        log_filepath = os.path.join(log_dir, f'{timestamp}.log')
+        # 파일 핸들러
+        fh = logging.FileHandler(log_filepath)
+        fh.setFormatter(fmt)
+        logger.addHandler(fh)
+
+        # 콘솔 핸들러 (터미널 출력)
+        ch = logging.StreamHandler(sys.stdout)
+        logger.addHandler(ch)
+
+    return logger
diff --git a/src/util/negativesampler.py b/src/util/negativesampler.py
index d3c95af..ae504da 100644
--- a/src/util/negativesampler.py
+++ b/src/util/negativesampler.py
@@ -2,7 +2,9 @@ from copy import deepcopy
 import numpy as np
 import pandas as pd
 import tqdm
+import logging
 
+logger = logging.getLogger("svdfm_test")
 class NegativeSampler:
     #  takes input of original dataframe and movie info
     #  make a function that returns negative sampled data
@@ -46,7 +48,7 @@ class NegativeSampler:
         df['user_frequency'] = df.groupby('user_id')['user_id'].transform('count')
         df['item_frequency'] = df.groupby('item_id')['item_id'].transform('count')
 
-        print("Negative Sampling Started")
+        logger.info("Negative Sampling Started")
 
         for customer in tqdm.tqdm(unique_customers[:]):
 
@@ -92,5 +94,5 @@ class NegativeSampler:
         # original과 not_purchased_df 합쳐서 return
         to_return = pd.concat([self.original_df, not_purchased_df], axis=0, ignore_index=True)
         to_return.drop(['user_frequency','item_frequency'], axis=1, inplace=True)
-        print("Negative Sampling Finished")
+        logger.info("Negative Sampling Finished")
         return to_return
\ No newline at end of file
